{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "47748c0985b935b4e46d2a6c83af0eecf1516f65"
   },
   "source": [
    "# Project - Google Analytics Customer Revenue Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7fbc08fd2ec0ec81825c6d794576fbb7a06ff9c1"
   },
   "source": [
    "## Presenting the initial data: \n",
    "\n",
    "<b>Data Fields: </b>\n",
    "\n",
    "<b>fullVisitorIdv</b> - A unique identifier for each user of the Google Merchandise Store. <br>\n",
    "<b>channelGrouping</b> - The channel via which the user came to the Store.<br>\n",
    "<b>date</b> - The date on which the user visited the Store.<br>\n",
    "<b>device </b>- The specifications for the device used to access the Store.<br>\n",
    "<b>geoNetwork</b> - This section contains information about the geography of the user.<br>\n",
    "<b>sessionId</b> - A unique identifier for this visit to the store.<br>\n",
    "<b>socialEngagementType</b> - Engagement type, either \"Socially Engaged\" or \"Not Socially Engaged\".<br>\n",
    "<b>totals</b> - This section contains aggregate values across the session.<br>\n",
    "<b>trafficSource</b> - This section contains information about the Traffic Source from which the session originated.<br>\n",
    "<b>visitId</b> - An identifier for this session. This is part of the value usually stored as the _utmb cookie. This is only unique to the user. For a completely unique ID, you should use a combination of fullVisitorId and visitId.<br>\n",
    "<b>visitNumber</b> - The session number for this user. If this is the first session, then this is set to 1.<br>\n",
    "<b>visitStartTime</b> - The timestamp (expressed as POSIX time).<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d5db3cb7d849d9fad6163f3f8b1e671cd6f17b93"
   },
   "source": [
    "# Objectives: \n",
    "\n",
    "The main objectives of this project are :\n",
    "\n",
    "* Load the data so everything is in tabular format (some columns contain JSON so it you will need to find ways to separate those into independent columns)\n",
    "* Identify the variables that need special processing (removing or infering missing values, removing columns that don't contain useful information)\n",
    "* Run visualizations to better understand the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6ea6f1fa6570a74176e88faf94c3da6782824043"
   },
   "source": [
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import os\n",
    "import random \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json \n",
    "\n",
    "from pandas.io.json import json_normalize \n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from statsmodels.api import OLS\n",
    "\n",
    "\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5ea0bc7871409e13de182189ebc0f30ddf7f3573",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load data\n",
    "df = pd.read_csv(\"s3://full-stack-bigdata-datasets/Machine Learning SupervisÃ©/projects/preprocessing_linear_models/Google_dataset_sample.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deal with date\n",
    "def date_process(df):\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], format=\"%Y%m%d\")\n",
    "    df[\"_weekday\"] = df['date'].dt.weekday\n",
    "    df[\"_day\"] = df['date'].dt.day\n",
    "    df[\"_month\"] = df['date'].dt.month\n",
    "    df[\"_year\"] = df['date'].dt.year\n",
    "    df['_visitHour'] = (df['visitStartTime'].apply(lambda x: str(datetime.fromtimestamp(x).hour))).astype(int)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = date_process(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deal with missing values\n",
    "def NumericalColumns(df): \n",
    "    df['totals.pageviews'].fillna(1, inplace=True)\n",
    "    df['totals.newVisits'].fillna(0, inplace=True)\n",
    "    df['totals.bounces'].fillna(0, inplace=True)\n",
    "    df['trafficSource.isTrueDirect'].fillna(False, inplace=True)\n",
    "    df['trafficSource.adwordsClickInfo.isVideoAd'].fillna(True, inplace=True)\n",
    "    df[\"totals.transactionRevenue\"] = df[\"totals.transactionRevenue\"].fillna(0.0).astype(float) \n",
    "    df['totals.pageviews'] = df['totals.pageviews'].astype(int) \n",
    "    df['totals.newVisits'] = df['totals.newVisits'].astype(int)\n",
    "    df['totals.bounces'] = df['totals.bounces'].astype(int) \n",
    "    df[\"totals.hits\"] = df[\"totals.hits\"].astype(float)\n",
    "    df['totals.visits'] = df['totals.visits'].astype(int)\n",
    "\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize data\n",
    "def Normalizing(df):\n",
    "    df[\"totals.hits\"] =  (df['totals.hits'] - min(df['totals.hits'])) / (max(df['totals.hits'])  - min(df['totals.hits']))\n",
    "    df['totals.transactionRevenue'] = df_train['totals.transactionRevenue'].apply(lambda x: np.log10(x+1))\n",
    "\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = NumericalColumns(df_train)\n",
    "\n",
    "df_train = Normalizing(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up the data\n",
    "df_clean = df_train.drop([\"date\", \"sessionId\", \"visitId\", \"visitNumber\", \"visitStartTime\", \"geoNetwork.region\", \"geoNetwork.metro\", \"geoNetwork.city\", \"geoNetwork.networkDomain\",\n",
    "\"trafficSource.source\",\t\"trafficSource.medium\", \"trafficSource.isTrueDirect\",\t\"trafficSource.adwordsClickInfo.isVideoAd\",\t\"trafficSource.campaignCode\", \"geoNetwork.continent\",\t\"geoNetwork.subContinent\", \"_day\"], axis = 1)\n",
    "\n",
    "transform_to_string = [\"_weekday\", \"_month\", \"_year\", \"_visitHour\"]\n",
    "for col in transform_to_string:\n",
    "    df_clean[col] = df_clean[col].astype(str)\n",
    "\n",
    "df_id = df_clean[\"fullVisitorId\"]\n",
    "df_no_id = df_clean.drop([\"fullVisitorId\"], axis=1)\n",
    "\n",
    "object_variables = df_no_id.select_dtypes(include = \"object\")\n",
    "non_object_variables = df_no_id.select_dtypes(exclude = \"object\")\n",
    "\n",
    "category_to_replace = []\n",
    "for col in object_variables :\n",
    "    value_proportion_table = object_variables[col].value_counts()/len(object_variables)\n",
    "    columns_to_replace = [col for col in value_proportion_table.keys() if value_proportion_table[col]>0.01]\n",
    "    category_to_replace.append(columns_to_replace)\n",
    "\n",
    "for i, col in enumerate(object_variables.columns) :\n",
    "    object_variables[col] = np.where(object_variables[col].isin(category_to_replace[i]),object_variables[col], \"others\")\n",
    "\n",
    "df_no_id = pd.concat([object_variables,non_object_variables], axis=1)\n",
    "\n",
    "df_clean = pd.get_dummies(df_no_id, drop_first=True)\n",
    "df_clean[\"fullVisitorId\"] = df_id\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group and aggregate\n",
    "df_agg = df_clean.groupby(\"fullVisitorId\").sum()\n",
    "df_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate target and training variables\n",
    "y = df_agg[\"totals.transactionRevenue\"]\n",
    "X = df_agg.drop([\"totals.transactionRevenue\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize data\n",
    "sc = StandardScaler()\n",
    "\n",
    "X_train = pd.DataFrame(sc.fit_transform(X_train), columns = X_train.columns, index= X_train.index)\n",
    "X_test = pd.DataFrame(sc.fit_transform(X_test), columns = X_test.columns, index= X_test.index)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model\n",
    "X_train[\"constant\"] = 1\n",
    "X_test[\"constant\"] = 1\n",
    "model = OLS(y_train, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove highly correlated values\n",
    "corr = X.corr()\n",
    "high_corr = corr > 0.95\n",
    "high_corr_list = [(i,j) for i in range(corr.shape[0]) for j in range(corr.shape[0]) if i != j and high_corr.iloc[i,j]]\n",
    "high_corr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_keep = []\n",
    "unique_couples = []\n",
    "for couple in high_corr_list :\n",
    "    if (couple[1],couple[0]) not in unique_couples:\n",
    "        unique_couples.append(couple)\n",
    "        no_keep.append(couple[1])\n",
    "\n",
    "X_train = X_train.drop(X_train.columns[no_keep], axis=1)\n",
    "X_test = X_test.drop(X_test.columns[no_keep], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.columns[no_keep])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OLS(y_train, X_train)\n",
    "model_fit = model.fit()\n",
    "model_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lasso and grid search\n",
    "params = {'alpha' : [10**(-a) for a in range(10)]}\n",
    "lasso = Lasso()\n",
    "grid = GridSearchCV(lasso,param_grid=params, cv = 3, verbose=1)\n",
    "\n",
    "grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid.best_estimator_\n",
    "print(\"Score on the train set :\", best_model.score(X_train,y_train))\n",
    "print(\"Score on the test set :\", best_model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"columns that have been removed with lasso : \", X_train.columns[best_model.coef_==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"columns that have been kept with lasso : \", X_train.columns[best_model.coef_!=0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try ridge\n",
    "params = {'alpha':np.arange(0,1000,100)} \n",
    "ridge = Ridge()\n",
    "\n",
    "grid = GridSearchCV(ridge, params, cv=3, verbose = 1)\n",
    "grid_fit = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid.best_estimator_\n",
    "print(\"Score on the train set :\", best_model.score(X_train,y_train))\n",
    "print(\"Score on the test set :\", best_model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
